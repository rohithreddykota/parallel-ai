{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start a Dask Client with 8 workers, each using a single thread (and thus a single CPU)\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "print(\"Dashboard link:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the diagnostics tools\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "def profile(func):\n",
    "    \"\"\"Profile the function using the Dask Profiler, ResourceProfiler, and CacheProfiler\n",
    "\n",
    "    Args:\n",
    "        func (function): The function to profile\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "            result = func(*args, **kwargs)\n",
    "        print(prof.results)\n",
    "        print(rprof.results)\n",
    "        print(cprof.results)\n",
    "        return result, prof, rprof, cprof\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/dask_ml/model_selection/_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = dd.read_csv(\"train_FE.csv\", blocksize='64MB')\n",
    "train=train.drop('Unnamed: 0', axis=1)\n",
    "X = train[train.columns[3:]]\n",
    "X = X.drop([\"pickup_day_of_week\", \"euc_distance\"], axis=1)\n",
    "y = train[train.columns[1:2]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Start a Dask client\n",
    "# client = Client()\n",
    "\n",
    "# Load the dataset using Dask\n",
    "train = dd.read_csv(\"train_FE.csv\")\n",
    "train = train.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Preprocessing with Dask\n",
    "X = train[train.columns[3:]]\n",
    "X = X.drop([\"pickup_day_of_week\", \"pickup_date\", \"euc_distance\"], axis=1)\n",
    "y = train[train.columns[1:2]]\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Ensuring computations are triggered for shape\n",
    "X_train_shape = X_train.shape.compute()\n",
    "y_train_shape = y_train.shape.compute()\n",
    "X_test_shape = X_test.shape.compute()\n",
    "y_test_shape = y_test.shape.compute()\n",
    "\n",
    "print(X_train_shape, y_train_shape)\n",
    "print(X_test_shape, y_test_shape)\n",
    "\n",
    "###################\n",
    "# Linear regression\n",
    "###################\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation scores\n",
    "scores = cross_val_score(lr, X_train, y_train, cv=6).compute()\n",
    "\n",
    "# Making predictions\n",
    "lr_y_pred = cross_val_predict(lr, X_test, y_test, cv=6).compute()\n",
    "\n",
    "# Coefficients\n",
    "# Note: Dask's LinearRegression doesn't provide coef_ directly like sklearn, might need to adjust based on the Dask ML version or model specifics.\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, lr_y_pred))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, lr_y_pred)\n",
    "\n",
    "# Calculate Variance Score (R^2)\n",
    "variance_score = r2_score(y_test, lr_y_pred)\n",
    "\n",
    "# Computing metrics (Ensure to trigger computation if they're dask objects)\n",
    "print(f\"RMSE: {rmse.compute()}\")\n",
    "print(f\"Mean squared error: {mse.compute():.2f}\")\n",
    "print(f\"Variance score: {variance_score.compute():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load your dataset\n",
    "df = dd.read_csv(\"train_FE.csv\", blocksize='64MB')\n",
    "\n",
    "# Preprocessing steps\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "X = df.drop(['fare_amount'], axis=1)\n",
    "y = df['fare_amount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/dask/array/core.py:3469: UserWarning: Passing an object to dask.array.from_array which is already a Dask collection. This can lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you need to convert to Dask Arrays for sklearn compatibility\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_dask \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust chunk sizes based on your dataset and available memory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y_dask \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mfrom_array(y, chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_computed \u001b[38;5;241m=\u001b[39m X_dask\u001b[38;5;241m.\u001b[39mcompute()  \u001b[38;5;66;03m# Converts Dask Array to NumPy array\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/dask/array/core.py:3483\u001b[0m, in \u001b[0;36mfrom_array\u001b[0;34m(x, chunks, name, lock, asarray, fancy, getitem, meta, inline_array)\u001b[0m\n\u001b[1;32m   3478\u001b[0m     asarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array_function__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3480\u001b[0m previous_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3482\u001b[0m chunks \u001b[38;5;241m=\u001b[39m normalize_chunks(\n\u001b[0;32m-> 3483\u001b[0m     chunks, x\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m, previous_chunks\u001b[38;5;241m=\u001b[39mprevious_chunks\n\u001b[1;32m   3484\u001b[0m )\n\u001b[1;32m   3486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3487\u001b[0m     token \u001b[38;5;241m=\u001b[39m tokenize(x, chunks, lock, asarray, fancy, getitem, inline_array)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/dask/dataframe/core.py:4951\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4949\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m   4950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4951\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you need to convert to Dask Arrays for sklearn compatibility\n",
    "import dask.array as da\n",
    "\n",
    "X_dask = da.from_array(X, chunks=(10000, X.shape[1]))  # Adjust chunk sizes based on your dataset and available memory\n",
    "y_dask = da.from_array(y, chunks=10000)\n",
    "\n",
    "X_computed = X_dask.compute()  # Converts Dask Array to NumPy array\n",
    "y_computed = y_dask.compute()  # Converts Dask Array to NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "# Note: Ensure your data size is manageable to fit into memory for this step\n",
    "scores = cross_val_score(lr, X_computed, y_computed, cv=6)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Average Score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_ml.model_selection.train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from dask_ml.model_selection import train_test_split, cross_val_score, cross_val_predict\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_test_split\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtts\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml.model_selection.train_test_split'"
     ]
    }
   ],
   "source": [
    "# from dask_ml.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "import dask_ml.model_selection.train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/dask_ml/model_selection/_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/dask_xgboost/__init__.py:7: UserWarning: Dask-XGBoost has been deprecated and is no longer maintained. The functionality of this project has been included directly in XGBoost. To use Dask and XGBoost together, please use ``xgboost.dask`` instead https://xgboost.readthedocs.io/en/latest/tutorials/dask.html.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.metrics import mean_squared_error, r2_score\n",
    "import dask.array as da\n",
    "from distributed import default_client\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from dask.distributed import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Close the existing client\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If you also want to shut down the scheduler and all workers explicitly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# default_client().cluster.close()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Close the existing client\n",
    "client.close()\n",
    "\n",
    "# If you also want to shut down the scheduler and all workers explicitly\n",
    "# default_client().cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure the new number of workers and threads per worker\n",
    "n_workers = 4  # For example, changing to 8 workers\n",
    "threads_per_worker = 2  # And adjusting threads per worker\n",
    "\n",
    "# Start a new client with the updated configuration\n",
    "client = Client(n_workers=n_workers, threads_per_worker=threads_per_worker)\n",
    "print(\"Dashboard link:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dd.read_csv(\"train_FE.csv\", blocksize='64MB')\n",
    "train = train.drop(\"Unnamed: 0\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[train.columns[3:]]\n",
    "X = X.drop([\"pickup_day_of_week\", \"euc_distance\"], axis=1)\n",
    "y = train['fare_amount'].to_frame()  # Assuming 'fare_amount' is your target column\n",
    "\n",
    "# Convert to dask arrays if necessary for some models\n",
    "X, y = X.to_dask_array(lengths=True), y.to_dask_array(lengths=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, lr_y_pred)\n",
    "rmse = da.sqrt(mse).compute()  # Compute RMSE from MSE\n",
    "r2 = r2_score(y_test, lr_y_pred).compute()  # Compute R²\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.74191947 0.75792071 0.74513824 0.77729981 0.76624039]\n",
      "Average score: 0.757703724491168\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "with joblib.parallel_backend('dask'):\n",
    "    scores = cross_val_score(lr, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Average score: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 49711 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://127.0.0.1:49711/status\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.metrics import mean_squared_error, r2_score\n",
    "import dask.array as da\n",
    "from distributed import default_client\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "# from dask_ml.ensemble import RandomForestRegressor\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Configure the new number of workers and threads per worker\n",
    "n_workers = 4  # For example, changing to 8 workers\n",
    "threads_per_worker = 2  # And adjusting threads per worker\n",
    "\n",
    "# Start a new client with the updated configuration\n",
    "client = Client(n_workers=n_workers, threads_per_worker=threads_per_worker)\n",
    "print(\"Dashboard link:\", client.dashboard_link)\n",
    "train = dd.read_csv(\"train_FE.csv\", blocksize='64MB')\n",
    "train = train.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "X = train[train.columns[3:]]\n",
    "X = X.drop([\"pickup_day_of_week\", \"euc_distance\"], axis=1)\n",
    "y = train['fare_amount'].to_frame()  # Assuming 'fare_amount' is your target column\n",
    "\n",
    "# Convert to dask arrays if necessary for some models\n",
    "X, y = X.to_dask_array(lengths=True), y.to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, lr_y_pred)\n",
    "rmse = da.sqrt(mse).compute()  # Compute RMSE from MSE\n",
    "r2 = r2_score(y_test, lr_y_pred).compute()  # Compute R²\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² score: {r2}\")\n",
    "\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    scores = cross_val_score(lr, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Average score: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.740503063135383\n",
      "Mean squared error: 32.95337541786671\n",
      "Variance score: 0.6354104415100583\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# Since DecisionTreeRegressor does not natively support Dask, \n",
    "# the fitting process here won't be parallelized beyond what sklearn internally supports (e.g., through joblib for some operations)\n",
    "dt.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "# Parallelize cross-validation using Dask with Joblib\n",
    "with joblib.parallel_backend('dask'):\n",
    "    scores = cross_val_score(dt, X_train.compute(), y_train.compute(), cv=6)\n",
    "    # cross_val_predict is not demonstrated to be parallelized directly via Dask as its primary goal isn't scoring or hyperparameter tuning\n",
    "    dt_y_pred = cross_val_predict(dt, X_test.compute(), y_test.compute(), cv=6)\n",
    "\n",
    "# Compute metrics\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test.compute(), dt_y_pred))\n",
    "r2_dt = r2_score(y_test.compute(), dt_y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse_dt}\")\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test.compute(), dt_y_pred)}\")\n",
    "print(f\"Variance score: {r2_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddykota/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.8566924153128532\n",
      "Mean squared error: 14.874076386331689\n",
      "Variance score: 0.8354361920782742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    rf.fit(X_train.compute(), y_train.compute())  # `.compute()` ensures data is in memory. Be mindful of large datasets.\n",
    "\n",
    "# Predict in a parallel computation context\n",
    "with joblib.parallel_backend('dask'):\n",
    "    rf_y_pred = rf.predict(X_test.compute())  # Ensure X_test is computed if it's a Dask object.\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test.compute(), rf_y_pred))  # Ensure y_test is computed if it's a Dask object.\n",
    "r2_rf = r2_score(y_test.compute(), rf_y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse_rf}\")\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test.compute(), rf_y_pred)}\")\n",
    "print(f\"Variance score: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import dask_xgboost as dxgb\n",
    "import xgboost as xgb\n",
    "from dask_ml.model_selection import GridSearchCV, train_test_split\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import numpy as np\n",
    "\n",
    "xgbr = dxgb.XGBRegressor()\n",
    "\n",
    "# Convert to Dask Array if your data is not already in this format\n",
    "# X_train, y_train = da.rechunk(X_train, chunks=(1000, X_train.shape[1])), da.from_array(y_train, chunks=1000)\n",
    "\n",
    "# Train model\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgbr_y_pred = xgbr.predict(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [7],\n",
    "    # \"eta\": [0.2],\n",
    "    \"objective\": [\"reg:squarederror\"],\n",
    "    \"eval_metric\": [\"rmse\"],\n",
    "    \"learning_rate\": [0.5],\n",
    "}\n",
    "\n",
    "# Note: dask_ml's GridSearchCV does not require the `n_jobs` parameter as parallelism is managed by Dask\n",
    "grid_search = GridSearchCV(estimator=xgbr, param_grid=params, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train_dask)\n",
    "\n",
    "xgbr2_y_pred = grid_search.predict(X_test.compute())\n",
    "\n",
    "# Ensure y_test is a NumPy array for comparison\n",
    "y_test_computed = y_test.compute()\n",
    "\n",
    "rmse_xgbr2 = np.sqrt(mean_squared_error(y_test_computed, xgbr2_y_pred))\n",
    "print(f\"RMSE: {rmse_xgbr2}\")\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test_computed, xgbr2_y_pred)}\")\n",
    "print(f\"Variance score: {r2_score(y_test_computed, xgbr2_y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=8, threads_per_worker=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard link:\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import dask_xgboost as dxgb\n",
    "import xgboost as xgb\n",
    "from dask_ml.model_selection import GridSearchCV, train_test_split\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import numpy as np\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "from dask_ml.metrics import mean_squared_error, r2_score\n",
    "import dask.array as da\n",
    "from distributed import default_client\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "\n",
    "# from dask_ml.ensemble import RandomForestRegressor\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import dask_xgboost as dxgb\n",
    "import xgboost as xgb\n",
    "from dask_ml.model_selection import GridSearchCV, train_test_split\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask.distributed import Client\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dd.read_csv(\"train_FE.csv\", blocksize=\"64MB\")\n",
    "train = train.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "X = train[train.columns[3:]]\n",
    "X = X.drop([\"pickup_day_of_week\", \"euc_distance\"], axis=1)\n",
    "y = train[\"fare_amount\"].to_frame()  # Assuming 'fare_amount' is your target column\n",
    "\n",
    "# Convert to dask arrays if necessary for some models\n",
    "X, y = X.to_dask_array(lengths=True), y.to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "# Convert to Dask Array if your data is not already in this format\n",
    "# X_train, y_train = da.rechunk(X_train, chunks=(1000, X_train.shape[1])), da.from_array(y_train, chunks=1000)\n",
    "\n",
    "# Train model\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgbr_y_pred = xgbr.predict(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [7],\n",
    "    \"objective\": [\"reg:squarederror\"],\n",
    "    \"eval_metric\": [\"rmse\"],\n",
    "    \"learning_rate\": [1],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgbr, param_grid=params, cv=4)\n",
    "\n",
    "# rechunking the data\n",
    "X_train = da.rechunk(X_train, chunks=(chunks, X_train.shape[1]))\n",
    "y_train = da.rechunk(y_train, chunks=chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgbr2_y_pred = grid_search.predict(X_test.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure y_test is a NumPy array for comparison\n",
    "y_test_computed = y_test.compute()\n",
    "\n",
    "rmse_xgbr2 = np.sqrt(mean_squared_error(y_test_computed, xgbr2_y_pred))\n",
    "print(f\"RMSE: {rmse_xgbr2}\")\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test_computed, xgbr2_y_pred)}\")\n",
    "print(f\"Variance score: {r2_score(y_test_computed, xgbr2_y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
